{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_files(src, train, valid, ratio=.8):\n",
    "    \n",
    "    # make directory everytime\n",
    "    os.makedirs(src, exist_ok=True)\n",
    "    os.makedirs(train, exist_ok=True)\n",
    "    os.makedirs(valid, exist_ok=True)\n",
    "    \n",
    "    src_files = os.listdir(src)\n",
    "    for idx, file_name in enumerate(src_files):\n",
    "        full_file_name = os.path.join(src, file_name)\n",
    "        if os.path.isfile(full_file_name):\n",
    "                if idx < len(src_files)*ratio:\n",
    "                    shutil.copy(full_file_name, train)\n",
    "                else:\n",
    "                    shutil.copy(full_file_name, valid)\n",
    "                    \n",
    "# keep dir clean before copy files\n",
    "sub_src = '../data/lenses/sub'\n",
    "sub_train = '../data/lenses_train/sub'\n",
    "sub_valid = '../data/lenses_valid/sub'\n",
    "split_files(sub_src, sub_train, sub_valid, .8)\n",
    "\n",
    "no_sub_src = '../data/lenses/no_sub'\n",
    "no_sub_train = '../data/lenses_train/no_sub'\n",
    "no_sub_valid = '../data/lenses_valid/no_sub'\n",
    "split_files(no_sub_src, no_sub_train, no_sub_valid, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(150),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "train_dataset = datasets.ImageFolder(root='../data/lenses_train',\n",
    "                                           transform=data_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "\n",
    "valid_dataset = datasets.ImageFolder(root='../data/lenses_valid',\n",
    "                                           transform=data_transform)\n",
    "valid_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                             batch_size=8, shuffle=True,\n",
    "                                             num_workers=4)\n",
    "\n",
    "classes = ('no_sub', 'sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sub      sub   no_sub   no_sub   no_sub   no_sub      sub      sub\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join('%8s' % classes[labels[j]] for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LensesCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 34 * 34, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), 16 * 34 * 34)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01,\\\n",
    "#                                         steps_per_epoch=len(train_loader),\\\n",
    "#                                         epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.078\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "#         scheduler.step()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './lenses_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(valid_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))\n",
    "\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in valid_loader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
